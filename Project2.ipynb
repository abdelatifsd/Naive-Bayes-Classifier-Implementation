{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes classifier for email classification\n",
    "# ----\n",
    "# Set Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/Users/aboud/Desktop/train\" # Training directory\n",
    "test_dir = \"/Users/aboud/Desktop/test\" # Testing directory\n",
    "stop_words_dir = \"/Users/aboud/Desktop/English-Stop-Words.txt\" # stop words directory\n",
    "output_dir = \"/Users/aboud/Desktop/\" # Output destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import numpy as np\n",
    "import re,os,math\n",
    "\n",
    "os.chdir(train_dir)\n",
    "stop_words = open(stop_words_dir,\"r\")\n",
    "stop_words = stop_words.read()\n",
    "stop_words = re.split('[^a-zA-Z]',stop_words)\n",
    "\n",
    "class EmailNaiveBayesClassifier(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.words_ham_freq = {}\n",
    "        self.words_spam_freq = {}\n",
    "        self.word_given_ham_freq = {}\n",
    "        self.word_given_spam_freq = {}\n",
    "        self.word_given_ham_prob = {}\n",
    "        self.word_given_spam_prob = {}\n",
    "        self.ham_prior = 1000/1997\n",
    "        self.spam_prior = 997/1997\n",
    "        self.ham_total_words = 0\n",
    "        self.spam_total_words = 0\n",
    "        self.shared_vocab = []\n",
    "        self.predictions = []\n",
    "        self.labels = []\n",
    "        \n",
    "    def generate_word_ham_freq(self,string):\n",
    "        for word in (string):\n",
    "            if word in self.words_ham_freq:\n",
    "                self.words_ham_freq[word]+=1\n",
    "            else:\n",
    "                self.words_ham_freq[word]=1\n",
    "                \n",
    "    def generate_word_spam_freq(self,string):\n",
    "        for word in (string):\n",
    "            if word in self.words_spam_freq:\n",
    "                self.words_spam_freq[word]+=1\n",
    "            else:\n",
    "                self.words_spam_freq[word]=1\n",
    "    \n",
    "    def find_shared_vocab(self):\n",
    "        for word in self.words_ham_freq:\n",
    "            if word in self.words_spam_freq:\n",
    "                self.shared_vocab.append(word)\n",
    "                \n",
    "    def update_ham_and_spam_freq(self):\n",
    "        for word in self.words_ham_freq:\n",
    "            if word in self.shared_vocab:\n",
    "                self.word_given_ham_freq[word] = self.words_ham_freq[word]\n",
    "                \n",
    "        for word in self.words_spam_freq:\n",
    "            if word in self.shared_vocab:\n",
    "                self.word_given_spam_freq[word] = self.words_spam_freq[word]\n",
    "                \n",
    "\n",
    "    def generate_total_words(self):\n",
    "    \n",
    "        for word in self.word_given_ham_freq:\n",
    "            self.ham_total_words = self.ham_total_words + self.word_given_ham_freq[word]\n",
    "        \n",
    "        for word in self.word_given_spam_freq:\n",
    "            self.spam_total_words = self.spam_total_words + self.word_given_spam_freq[word]\n",
    "        \n",
    "\n",
    "    def generate_conditional_probabilities(self,smoothing):\n",
    "        \n",
    "        for word in self.word_given_ham_freq:\n",
    "            self.word_given_ham_freq[word]+=smoothing\n",
    "            self.word_given_ham_prob[word]= self.word_given_ham_freq[word] /(self.ham_total_words + len(self.shared_vocab)*smoothing)\n",
    "\n",
    "\n",
    "        for word in self.word_given_spam_freq:\n",
    "            self.word_given_spam_freq[word]+=smoothing\n",
    "            self.word_given_spam_prob[word]= self.word_given_spam_freq[word] /(self.spam_total_words + len(self.shared_vocab)*smoothing)\n",
    "        \n",
    "    def predict(self,ham,spam):\n",
    "        if ham > spam:\n",
    "            self.predictions.append(\"ham\")\n",
    "            return \"ham\"\n",
    "        else:\n",
    "            self.predictions.append(\"spam\")\n",
    "            return \"spam\"\n",
    "        \n",
    "    def train(self):\n",
    "        self.find_shared_vocab() # Computes union\n",
    "        self.update_ham_and_spam_freq() # updates dictionaries so they hold words in union only\n",
    "        self.generate_total_words() # Counts total words/class\n",
    "        self.generate_conditional_probabilities(0.5) # Computes conditional probabilities\n",
    "    \n",
    "    \"METRICS\"\n",
    "    \n",
    "    \n",
    "    def metrics(self):\n",
    "        \n",
    "        cm = np.zeros((2,2),dtype=object)\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "        FP = 0\n",
    "        \n",
    "        for i in range(len(self.labels)):\n",
    "            if self.labels[i] == \"ham\" and  self.predictions[i] == \"ham\":\n",
    "                TP+=1\n",
    "            if self.labels[i] == \"spam\" and  self.predictions[i] == \"spam\":\n",
    "                TN+=1\n",
    "            elif self.labels[i] == \"ham\" and self.predictions[i] == \"spam\":\n",
    "                FN+=1\n",
    "            elif self.labels[i] == \"spam\" and self.predictions[i] == \"ham\":\n",
    "                FP+=1\n",
    "        \n",
    "        accuracy = (TP + TN)/len(self.labels)\n",
    "        percision = (TP) / (TP + FP)\n",
    "        recall = (TP) / (TP + FN)\n",
    "      \n",
    "        f1_score =  2*((percision*recall) / (percision+recall))\n",
    "        \n",
    "        accuracy_ham = TP / 400\n",
    "        accuracy_spam = TN / 400\n",
    "        \n",
    "        percision_ham = TP / (TP + FP)\n",
    "        percision_spam = TN / (TN + FN)\n",
    "        \n",
    "        recall_ham =  TP / (TP+FN)\n",
    "        recall_spam = TN / (TN+FP)\n",
    "        \n",
    "        f1_score_ham = 2* ((percision_ham*recall_ham)/(percision_ham+recall_ham))\n",
    "        f1_score_spam = 2* ((percision_spam*recall_spam)/(percision_spam+recall_spam))\n",
    "        \n",
    "      \n",
    "        \n",
    "        print(\"CONFUSION-MATRIX:\")\n",
    "        cm[0,0]= \"TP:\"+str(TP)\n",
    "        cm[0,1]= \"FP:\"+str(FP)\n",
    "        cm[1,0]= \"FN:\"+str(FN)\n",
    "        cm[1,1]= \"TN:\"+str(TN)\n",
    "        print(cm)\n",
    "        print(\"-------------------------------------------\")\n",
    "        print(\"METRICS:\")\n",
    "        print(\"Class Ham: \")\n",
    "        print(\"Accuracy: \" + str(accuracy_ham*100) + \"%\")\n",
    "        print(\"Percision: \" + str(percision_ham*100) + \"%\")\n",
    "        print(\"Recall: \" + str(recall_ham*100) + \"%\")\n",
    "        print(\"F1-Score: \" + str(f1_score_ham*100)+\"%\")\n",
    "        print(\"Class Spam: \")\n",
    "        print(\"Accuracy: \" + str(accuracy_spam*100) + \"%\")\n",
    "        print(\"Percision: \" + str(percision_spam*100) + \"%\")\n",
    "        print(\"Recall: \" + str(recall_spam*100) + \"%\")\n",
    "        print(\"F1-Score: \" + str(f1_score_spam*100)+\"%\")\n",
    "        print(\"-------------------------------------------\")\n",
    "        print(\"AVERAGED RESULTS:\")\n",
    "        print(\"ACCURACY:\"+str((accuracy)*100) +\"%\")\n",
    "        print(\"PERCISION:\"+str((percision)*100) +\"%\")\n",
    "        print(\"RECALL:\"+str((recall)*100) +\"%\")\n",
    "        print(\"F1-Score:\"+str((f1_score)*100) +\"%\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \"PARSING & WRITING\"\n",
    "    \n",
    "    def parse_txt(self):\n",
    "        for txt in os.listdir():\n",
    "            if txt.split(\".\")[1] == \"txt\" and txt.split(\"-\")[1] == \"ham\":\n",
    "                    with open(txt, 'r', encoding='utf-8',errors='ignore') as file:\n",
    "                        words = (file.read())\n",
    "                        words = re.split('[^a-zA-Z]',words)\n",
    "                        words = [word.lower() for word in words if word!=\"\"]\n",
    "                        self.generate_word_ham_freq(words)\n",
    "                        file.close()\n",
    "            elif txt.split(\".\")[1] == \"txt\" and txt.split(\"-\")[1] == \"spam\":\n",
    "                if txt.split(\"-\")[1] != \"Stop\":\n",
    "                    with open(txt, 'r', encoding='utf-8',errors='ignore') as file:\n",
    "                        words = (file.read())\n",
    "                        words = re.split('[^a-zA-Z]',words)\n",
    "                        words = [word.lower() for word in words if word!=\"\"]\n",
    "                        self.generate_word_spam_freq(words)\n",
    "                        file.close()\n",
    "                        \n",
    "    def parse_txt_swords(self,stop_words):\n",
    "        for txt in os.listdir():\n",
    "            if txt.split(\".\")[1] == \"txt\" and txt.split(\"-\")[1] == \"ham\":\n",
    "                    with open(txt, 'r', encoding='utf-8',errors='ignore') as file:\n",
    "                        words = (file.read())\n",
    "                        words = re.split('[^a-zA-Z]',words)\n",
    "                        words = [word.lower() for word in words if word!=\"\"]\n",
    "                        words = [word for word in words if word not in stop_words]\n",
    "                        self.generate_word_ham_freq(words)\n",
    "                        file.close()\n",
    "            elif txt.split(\".\")[1] == \"txt\" and txt.split(\"-\")[1] == \"spam\":\n",
    "                    with open(txt, 'r', encoding='utf-8',errors='ignore') as file:\n",
    "                        words = (file.read())\n",
    "                        words = re.split('[^a-zA-Z]',words)\n",
    "                        words = [word.lower() for word in words if word!=\"\"]\n",
    "                        words = [word for word in words if word not in stop_words]\n",
    "                        self.generate_word_spam_freq(words)\n",
    "                        file.close()\n",
    "                        \n",
    "    def parse_txt_wlength(self):\n",
    "        \n",
    "        for txt in os.listdir():\n",
    "            if txt.split(\".\")[1] == \"txt\" and txt.split(\"-\")[1] == \"ham\":\n",
    "                    with open(txt, 'r', encoding='utf-8',errors='ignore') as file:\n",
    "                        words = (file.read())\n",
    "                        words = re.split('[^a-zA-Z]',words)\n",
    "                        words = [word.lower() for word in words if word!=\"\"]\n",
    "                        words = [word for word in words if len(word) > 3 and len(word) <9]\n",
    "                        self.generate_word_ham_freq(words)\n",
    "                        file.close()\n",
    "            elif txt.split(\".\")[1] == \"txt\" and txt.split(\"-\")[1] == \"spam\":\n",
    "                    with open(txt, 'r', encoding='utf-8',errors='ignore') as file:\n",
    "                        words = (file.read())\n",
    "                        words = re.split('[^a-zA-Z]',words)\n",
    "                        words = [word.lower() for word in words if word!=\"\"]\n",
    "                        words = [word for word in words if len(word) > 3 and len(word) <9]\n",
    "                        self.generate_word_spam_freq(words)\n",
    "                        file.close()\n",
    "    \n",
    "    def parse_txt_all(self,stop_words):\n",
    "        for txt in os.listdir():\n",
    "            if txt.split(\".\")[1] == \"txt\" and txt.split(\"-\")[1] == \"ham\":\n",
    "                    with open(txt, 'r', encoding='utf-8',errors='ignore') as file:\n",
    "                        words = (file.read())\n",
    "                        words = re.split('[^a-zA-Z]',words)\n",
    "                        words = [word.lower() for word in words if word!=\"\"]\n",
    "                        words = [word for word in words if word not in stop_words]\n",
    "                        words = [word for word in words if len(word) > 3 and len(word) <9]\n",
    "                        self.generate_word_ham_freq(words)\n",
    "                        file.close()\n",
    "            elif txt.split(\".\")[1] == \"txt\" and txt.split(\"-\")[1] == \"spam\":\n",
    "                    with open(txt, 'r', encoding='utf-8',errors='ignore') as file:\n",
    "                        words = (file.read())\n",
    "                        words = re.split('[^a-zA-Z]',words)\n",
    "                        words = [word.lower() for word in words if word!=\"\"]\n",
    "                        words = [word for word in words if word not in stop_words]\n",
    "                        words = [word for word in words if len(word) > 3 and len(word) <9]\n",
    "                        self.generate_word_spam_freq(words)\n",
    "                        file.close()\n",
    "        \n",
    "    def write_output_model(self,txt):\n",
    "        output = open(os.path.join(output_dir,txt),\"w\")\n",
    "        for i,word in enumerate(self.shared_vocab):\n",
    "            output.write(str(i+1) + \"  \")\n",
    "            output.write(word + \"  \")\n",
    "            output.write(str(self.word_given_ham_freq[word]) + \"  \")\n",
    "            output.write(str( round(float(self.word_given_ham_prob[word]),8) ) + \"  \")\n",
    "            output.write(str(self.word_given_spam_freq[word]) + \"  \")\n",
    "            output.write(str(round(float(self.word_given_spam_prob[word]),8) ) + \"  \")\n",
    "            output.write(\"\\n\")\n",
    "\n",
    "        output.close()\n",
    "    \n",
    "    \"MODEL TESTING + WRITING\"\n",
    "    def write_result_output(self,otxt):\n",
    "        counter = 1\n",
    "        labels = []\n",
    "        output = open(os.path.join(output_dir,otxt),\"w\")\n",
    "\n",
    "        for txt in os.listdir():\n",
    "            if txt.split(\"-\")[1] == \"ham\" or txt.split(\"-\")[1] == \"spam\":\n",
    "                with open(txt, 'r', encoding='utf-8',errors='ignore') as file:\n",
    "                    self.labels.append(txt.split(\"-\")[1])\n",
    "                    words = (file.read())\n",
    "                    words = re.split('[^a-zA-Z]',words)\n",
    "                    words = [word.lower() for word in words if word!=\"\"]\n",
    "                    p_ham = 0\n",
    "                    p_spam = 0\n",
    "                    for word in words:\n",
    "                        if word in self.word_given_ham_freq:\n",
    "                            p_ham += np.log10(self.word_given_ham_prob[word])\n",
    "                            p_spam += np.log10(self.word_given_spam_prob[word])\n",
    "\n",
    "                    p_ham = np.log10(self.ham_prior) + (p_ham)\n",
    "                    p_spam = np.log10(self.spam_prior) + (p_spam)\n",
    "\n",
    "                    output.write(str(counter) + \"  \")\n",
    "                    output.write(txt + \"  \")\n",
    "                    output.write(self.predict(p_ham,p_spam) + \"  \")\n",
    "                    output.write(str(round(p_ham,5)) + \"  \")\n",
    "                    output.write(str(round(p_spam,5)) + \"  \")\n",
    "                    output.write(txt.split(\"-\")[1] + \"  \")\n",
    "                    output.write(\"\\n\")\n",
    "\n",
    "                    counter+=1\n",
    "\n",
    "        file.close()\n",
    "        output.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expirement 1 - baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbo = EmailNaiveBayesClassifier() # Naive bayes class object\n",
    "os.chdir(train_dir)\n",
    "nbo.parse_txt()\n",
    "nbo.train()\n",
    "nbo.write_output_model(\"model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(test_dir)\n",
    "if os.path.isfile(\".DS_Store\"):\n",
    "    os.remove(\".DS_Store\")\n",
    "nbo.write_result_output(\"baseline-result.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION-MATRIX:\n",
      "[['TP:394' 'FP:74']\n",
      " ['FN:6' 'TN:326']]\n",
      "-------------------------------------------\n",
      "METRICS:\n",
      "Class Ham: \n",
      "Accuracy: 98.5%\n",
      "Percision: 84.1880341880342%\n",
      "Recall: 98.5%\n",
      "F1-Score: 90.78341013824884%\n",
      "Class Spam: \n",
      "Accuracy: 81.5%\n",
      "Percision: 98.19277108433735%\n",
      "Recall: 81.5%\n",
      "F1-Score: 89.07103825136612%\n",
      "-------------------------------------------\n",
      "AVERAGED RESULTS:\n",
      "ACCURACY:90.0%\n",
      "PERCISION:84.1880341880342%\n",
      "RECALL:98.5%\n",
      "F1-Score:90.78341013824884%\n"
     ]
    }
   ],
   "source": [
    "results = (nbo.metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 - Stop word filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbo = EmailNaiveBayesClassifier() # Naive bayes class object\n",
    "os.chdir(train_dir)\n",
    "nbo.parse_txt_swords(stop_words)\n",
    "nbo.train()\n",
    "nbo.write_output_model(\"stopword-model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(test_dir)\n",
    "if os.path.isfile(\".DS_Store\"):\n",
    "    os.remove(\".DS_Store\")\n",
    "nbo.write_result_output(\"stopword-result.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION-MATRIX:\n",
      "[['TP:394' 'FP:71']\n",
      " ['FN:6' 'TN:329']]\n",
      "-------------------------------------------\n",
      "METRICS:\n",
      "Class Ham: \n",
      "Accuracy: 98.5%\n",
      "Percision: 84.73118279569893%\n",
      "Recall: 98.5%\n",
      "F1-Score: 91.09826589595376%\n",
      "Class Spam: \n",
      "Accuracy: 82.25%\n",
      "Percision: 98.2089552238806%\n",
      "Recall: 82.25%\n",
      "F1-Score: 89.52380952380953%\n",
      "-------------------------------------------\n",
      "AVERAGED RESULTS:\n",
      "ACCURACY:90.375%\n",
      "PERCISION:84.73118279569893%\n",
      "RECALL:98.5%\n",
      "F1-Score:91.09826589595376%\n"
     ]
    }
   ],
   "source": [
    "results = (nbo.metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirement 3 - Word length filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(train_dir)\n",
    "nbo = EmailNaiveBayesClassifier() # Naive bayes class object\n",
    "os.chdir(train_dir)\n",
    "nbo.parse_txt_wlength()\n",
    "nbo.train()\n",
    "nbo.write_output_model(\"wordlength-model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(test_dir)\n",
    "if os.path.isfile(\".DS_Store\"):\n",
    "    os.remove(\".DS_Store\")\n",
    "nbo.write_result_output(\"wordlength-result.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION-MATRIX:\n",
      "[['TP:392' 'FP:60']\n",
      " ['FN:8' 'TN:340']]\n",
      "-------------------------------------------\n",
      "METRICS:\n",
      "Class Ham: \n",
      "Accuracy: 98.0%\n",
      "Percision: 86.72566371681415%\n",
      "Recall: 98.0%\n",
      "F1-Score: 92.01877934272301%\n",
      "Class Spam: \n",
      "Accuracy: 85.0%\n",
      "Percision: 97.70114942528735%\n",
      "Recall: 85.0%\n",
      "F1-Score: 90.90909090909092%\n",
      "-------------------------------------------\n",
      "AVERAGED RESULTS:\n",
      "ACCURACY:91.5%\n",
      "PERCISION:86.72566371681415%\n",
      "RECALL:98.0%\n",
      "F1-Score:92.01877934272301%\n"
     ]
    }
   ],
   "source": [
    "results = (nbo.metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirement 4 - Stopwords + word length filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbo = EmailNaiveBayesClassifier() # Naive bayes class object\n",
    "os.chdir(train_dir)\n",
    "nbo.parse_txt_all(stop_words)\n",
    "nbo.train()\n",
    "nbo.write_output_model(\"stopword-model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(test_dir)\n",
    "if os.path.isfile(\".DS_Store\"):\n",
    "    os.remove(\".DS_Store\")\n",
    "nbo.write_result_output(\"stopword-result.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION-MATRIX:\n",
      "[['TP:392' 'FP:55']\n",
      " ['FN:8' 'TN:345']]\n",
      "-------------------------------------------\n",
      "METRICS:\n",
      "Class Ham: \n",
      "Accuracy: 98.0%\n",
      "Percision: 87.69574944071589%\n",
      "Recall: 98.0%\n",
      "F1-Score: 92.56198347107438%\n",
      "Class Spam: \n",
      "Accuracy: 86.25%\n",
      "Percision: 97.73371104815864%\n",
      "Recall: 86.25%\n",
      "F1-Score: 91.63346613545818%\n",
      "-------------------------------------------\n",
      "AVERAGED RESULTS:\n",
      "ACCURACY:92.125%\n",
      "PERCISION:87.69574944071589%\n",
      "RECALL:98.0%\n",
      "F1-Score:92.56198347107438%\n"
     ]
    }
   ],
   "source": [
    "results = (nbo.metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbo = EmailNaiveBayesClassifier() # Naive bayes class object\n",
    "os.chdir(train_dir)\n",
    "nbo.parse_txt()\n",
    "nbo.train()\n",
    "nbo.write_output_model(\"demo-model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(test_dir)\n",
    "if os.path.isfile(\".DS_Store\"):\n",
    "    os.remove(\".DS_Store\")\n",
    "nbo.write_result_output(\"demo-result.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = (nbo.metrics())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
